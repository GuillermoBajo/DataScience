{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project:** Pathology Detection in Crop Plants\n",
    "\n",
    "Members:\n",
    "* Domenico Azzarito​\n",
    "* Guillermo Bajo Laborda​\n",
    "* Laura Alejandra Moreno​\n",
    "* Arian Gharehmohammadzadehghashghaei​\n",
    "* Michele Pezza\n",
    "\n",
    "\n",
    "*Fundamentals of Data Science | Sapienza University of Rome*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN\n",
    "\n",
    "In this step, the image data has been loaded, and also a normaliation, resizing and augmentation process has been implemented.\n",
    "\n",
    "The key libraries used were TensorFlow for image processing, Pandas for handling the CSV files, and also os library. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab'], dtype='object')\n",
      "  image_id  healthy  multiple_diseases  rust  scab\n",
      "0  Train_0        0                  0     0     1\n",
      "1  Train_1        0                  1     0     0\n",
      "2  Train_2        1                  0     0     0\n",
      "3  Train_3        0                  0     1     0\n",
      "4  Train_4        1                  0     0     0\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "data_dir = 'images/'  \n",
    "sample_submission_csv = 'sample_submission.csv'\n",
    "test_csv = 'test.csv'\n",
    "train_csv = 'train.csv' \n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(train_df.columns)\n",
    "train_df.columns = train_df.columns.str.strip()\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded labels into arrays\n",
    "def con_process_labels(df):\n",
    "    labels = df[['healthy', 'multiple_diseases', 'rust', 'scab']].values\n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode function to load and preprocess the image file\n",
    "\n",
    "def decode_image(filename, label=None, image_size=(512, 512)):\n",
    "    filepath = tf.strings.join([data_dir, filename])  \n",
    "    bits = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    print(f\"decoded image shape: {image.shape}\")\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) \n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation to the images\n",
    "\n",
    "def data_augmentation(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Tensorflow dataset for testing\n",
    "\n",
    "def prepare_dataset(df, image_size=(512, 512), batch_size=32, augment=False, is_train=True):\n",
    "    file_paths = df['image_id'] + '.jpg'  \n",
    "    \n",
    "    if is_train:\n",
    "        labels = con_process_labels(df) \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        dataset = dataset.map(lambda x, y: decode_image(x, y, image_size))  \n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "        dataset = dataset.map(lambda x: decode_image(x, label=None, image_size=image_size))\n",
    "    \n",
    "    if augment and is_train:\n",
    "        dataset = dataset.map(data_augmentation)  \n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded image shape: (512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(test_csv)\n",
    "test_df.columns = test_df.columns.str.strip()  # Clean the column names for test set\n",
    "test_dataset = prepare_dataset(test_df, image_size=(512, 512), augment=False, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Softmax Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will focus on preprocessing image data and extracting additional features, such as color histograms, to enhance the model's performance. These features will be used alongside the images to train a convolutional neural network (CNN) model capable of classifying images into four categories: healthy, multiple_diseases, rust, and scab. We will then evaluate the model's performance using key metrics such as accuracy, confusion matrix, and F1-score. Ultimately, this approach will integrate both color histograms and images to achieve a more robust and accurate classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract histograms for R, G, B channels and vectorie them\n",
    "\n",
    "#This function extracts color histograms for the Red, Green, and Blue channels, \n",
    "#normalizes the values, and returns them as a combined vector\n",
    "def extract_color_histogram(image, bins=256):\n",
    "   \n",
    "    # Calculate histograms for each color channel (R, G, B)\n",
    "    r_hist = tf.histogram_fixed_width(image[..., 0], [0.0, 1.0], nbins=bins)\n",
    "    g_hist = tf.histogram_fixed_width(image[..., 1], [0.0, 1.0], nbins=bins)\n",
    "    b_hist = tf.histogram_fixed_width(image[..., 2], [0.0, 1.0], nbins=bins)\n",
    "    \n",
    "    # Normalize the histograms by dividing by the total number of pixels\n",
    "    total_pixels = tf.size(image[..., 0], out_type=tf.float32)  \n",
    "    r_hist = tf.cast(r_hist, tf.float32) / total_pixels\n",
    "    g_hist = tf.cast(g_hist, tf.float32) / total_pixels\n",
    "    b_hist = tf.cast(b_hist, tf.float32) / total_pixels\n",
    "    \n",
    "    # Concatenate the R, G, B histograms into a single vector\n",
    "    color_histogram = tf.concat([r_hist, g_hist, b_hist], axis=-1)\n",
    "\n",
    "    # Ensure values are within [0, 1]\n",
    "    color_histogram = tf.clip_by_value(color_histogram, 0.0, 1.0)\n",
    "\n",
    "    return color_histogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare color histograms for a dataset (no labels)\n",
    "\n",
    "#This function processes all images in the dataframe by reading and \n",
    "#resizing them, then extracting the color histograms\n",
    "def prepare_color_histograms(df, image_size=(500, 500)):\n",
    "    def process_image(filename):\n",
    "        filepath = tf.strings.join([data_dir, filename])  \n",
    "        bits = tf.io.read_file(filepath)\n",
    "        image = tf.image.decode_jpeg(bits, channels=3)  # Decode the image\n",
    "        image = tf.image.resize(image, image_size)  # Resize the image\n",
    "        color_histogram = extract_color_histogram(image)  # Extract the color histogram\n",
    "        return color_histogram\n",
    "\n",
    "    file_paths = df['image_id'] + '.jpg'  # Get the file paths for images\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "    dataset = dataset.map(process_image)  # Apply the process_image function to all file paths\n",
    "    histograms = [hist.numpy() for hist in dataset]  # Convert the results to numpy arrays\n",
    "    return histograms   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the one-hot encoded labels\n",
    "\n",
    "# This function converts the labels into one-hot encoded vectors\n",
    "def prepare_one_hot_labels(df):\n",
    "    labels = df[['healthy', 'multiple_diseases', 'rust', 'scab']].values\n",
    "    return tf.convert_to_tensor(labels, dtype=tf.float32)  # Convert to tensorflow tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the image dataset (for training)\n",
    "\n",
    "#This function prepares a dataset of images for training, decoding and resizing the images\n",
    "def prepare_image_dataset(df, image_size=(512, 512)):\n",
    "    def decode_image(filename, image_size=(512, 512)):\n",
    "        filepath = tf.strings.join([data_dir, filename])  \n",
    "        bits = tf.io.read_file(filepath)\n",
    "        image = tf.image.decode_jpeg(bits, channels=3)\n",
    "        image = tf.image.resize(image, image_size)  # Resize the image\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize image to [0, 1]\n",
    "        return image\n",
    "\n",
    "    file_paths = df['image_id'] + '.jpg'  # Get the file paths for images\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "    dataset = dataset.map(lambda x: decode_image(x, image_size))  # Apply image decoding function\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine images and histograms into a single dataset\n",
    "\n",
    "# This function combines the image dataset and histogram dataset into a single dataset, \n",
    "# along with the labels for training.\n",
    "def combine_image_and_histogram_datasets(image_dataset, histograms_dataset, labels_dataset):\n",
    "    combined_dataset = tf.data.Dataset.zip((image_dataset, histograms_dataset, labels_dataset))\n",
    "    combined_dataset = combined_dataset.batch(32).prefetch(tf.data.AUTOTUNE)  # Batch and prefetch for better performance\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "\n",
    "# This function builds the CNN model, using the input shape of the images.\n",
    "# It consists of convolutional layers followed by dense layers for classification\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Convolutional layers for feature extraction\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),  # Flatten the output of the convolutional layers\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')  # Output layer for the 4 possible classes\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "#This function compiles and trains the model on the given dataset\n",
    "def train_model(train_dataset):\n",
    "    # Build the model\n",
    "    model = build_model(input_shape=(512, 512, 3))  # Input shape is (512, 512, 3) for images\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
    "    \n",
    "    # Train the model on the combined dataset (images + histograms)\n",
    "    history = model.fit(train_dataset, epochs=10)  # Train for 10 epochs\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "#This function evaluates the trained model on the test dataset\n",
    "def evaluate_model(model, test_dataset):\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Additional metrics: Confusion matrix and F1-score\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "    # Get predictions and true labels\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_true = [labels.numpy() for _, labels in test_dataset]\n",
    "\n",
    "    # Convert predictions and labels to single values (from one-hot encoding)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(np.array(y_true), axis=1)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # F1-score\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1-score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
